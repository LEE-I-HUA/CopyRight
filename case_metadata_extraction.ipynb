{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ËºâÂÖ•Â•ó‰ª∂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_counsel_from_pdf(pdf_file_path):\n",
    "    with fitz.open(pdf_file_path) as doc:\n",
    "        full_text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "\n",
    "    counsel = \"\"\n",
    "    try:\n",
    "        # Â∞çÊñº Appellate Âà§Ê±∫ÔºöCounsel ÂèØËÉΩÂá∫ÁèæÂú® LexisNexis Headnotes ‰πãÂæå\n",
    "        match = re.search(\n",
    "            r'Counsel:\\s*(.+?)(?=\\n(?:HN\\d\\[|Headnotes|Judges?:|Opinion by:|Core Terms|Subsequent History:|Prior History:|Disposition:|$))',\n",
    "            full_text,\n",
    "            re.DOTALL\n",
    "        )\n",
    "        if match:\n",
    "            counsel = match.group(1).replace('\\n', ' ').strip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return counsel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counsel: [*1] For Warner Bros Home Entertainment Inc, Plaintiff: James Andrew Coombs, Nicole L Drey, J  Andrew Coombs APC, Glendale, CA.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"C:/Users/owoyi/Downloads/ÁõßÊïôÊéà/Â∞àÂà©ÂàÜÊûê/data/testing/cp13-38.pdf\"\n",
    "counsel_text = extract_counsel_from_pdf(pdf_path)\n",
    "print(\"Counsel:\", counsel_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the function with a fixed \"Opinion by\" extraction\n",
    "# This function extracts structured case-level metadata from a Lexis-style PDF\n",
    "def extract_case_metadata_fixed_opinion(pdf_file_path):\n",
    "    # Open the PDF and concatenate the full text of all pages\n",
    "    # full_text is used for regex-based global searches\n",
    "    with fitz.open(pdf_file_path) as doc:\n",
    "        full_text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "\n",
    "        # Extract block-level information from the first page only\n",
    "        # This is used as a fallback for structured elements (e.g., party names)\n",
    "        first_page_blocks = doc[0].get_text(\"dict\")[\"blocks\"]\n",
    "\n",
    "    # Helper function:\n",
    "    # Extract text following a given section header (pattern),\n",
    "    # stopping at the next detected section header in end_patterns.\n",
    "    def extract_with_regex(pattern, end_patterns, max_len=1000):\n",
    "        try:\n",
    "            match = re.search(\n",
    "                rf'{pattern}\\s+([\\s\\S]+?)(?=\\n(?:{\"|\".join(end_patterns)}))',\n",
    "                full_text\n",
    "            )\n",
    "            if match:\n",
    "                # Normalize whitespace and enforce a length limit\n",
    "                content = match.group(1).replace('\\n', ' ').strip()\n",
    "                return content if len(content) <= max_len else \"\"\n",
    "        except:\n",
    "            # Fail silently if regex fails\n",
    "            return \"\"\n",
    "        return \"\"\n",
    "\n",
    "    # --- Core Terms ---\n",
    "    # Extract the \"Core Terms\" section and split it into a list\n",
    "    # Stop extraction at common section boundaries\n",
    "    core_terms_raw = extract_with_regex(\n",
    "        \"Core Terms\",\n",
    "        [\"Counsel:\", \"LexisNexis\", \"HN\\\\d\\\\[\", \"Headnotes\", \"Opinion by:\", \"Judges?:\"]\n",
    "    )\n",
    "    core_terms = [term.strip() for term in core_terms_raw.split(',')] if core_terms_raw else []\n",
    "\n",
    "    # --- Judges ---\n",
    "    # Extract the list of judges associated with the opinion\n",
    "    judges = extract_with_regex(\n",
    "        \"Judges?:\",\n",
    "        [\"Opinion by:\", \"Core Terms\", \"Counsel:\"],\n",
    "        max_len=300\n",
    "    )\n",
    "\n",
    "    # --- Opinion by ---\n",
    "    # Extract the authoring judge using a simple one-line regex\n",
    "    # This avoids over-capturing multi-line sections\n",
    "    opinion_by = \"\"\n",
    "    try:\n",
    "        op_match = re.search(r'Opinion by:\\s*(.+)', full_text)\n",
    "        if op_match:\n",
    "            opinion_by = op_match.group(1).strip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # --- Prior History ---\n",
    "    # Extract procedural history before the current decision\n",
    "    prior_history = extract_with_regex(\n",
    "        \"Prior History:\",\n",
    "        [\"Disposition:\", \"Core Terms\", \"Judges?:\", \"Opinion by:\"],\n",
    "        max_len=1000\n",
    "    )\n",
    "\n",
    "    # --- Subsequent History ---\n",
    "    # Extract information about later procedural developments\n",
    "    subsequent_history = extract_with_regex(\n",
    "        \"Subsequent History:\",\n",
    "        [\"Prior History:\", \"Disposition:\", \"Core Terms\"],\n",
    "        max_len=1000\n",
    "    )\n",
    "\n",
    "    # --- Plaintiff / Defendant ---\n",
    "    # First attempt: extract party roles from the Counsel section\n",
    "    plaintiff_defendant = \"\"\n",
    "    try:\n",
    "        pd_counsel_match = re.search(\n",
    "            r'Counsel:\\s*\\[\\*?\\d*\\]\\s*For (.+?Plaintiff.*?)\\.\\s*',\n",
    "            full_text,\n",
    "            re.DOTALL\n",
    "        )\n",
    "        if pd_counsel_match:\n",
    "            plaintiff_defendant = pd_counsel_match.group(1).replace('\\n', ' ').strip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Fallback:\n",
    "    # If Counsel-based extraction fails, search the first page blocks\n",
    "    # for text containing both \"Plaintiff\" and \"Defendant\"\n",
    "    if not plaintiff_defendant:\n",
    "        for block in first_page_blocks:\n",
    "            if \"lines\" not in block:\n",
    "                continue\n",
    "            block_text = \" \".join(\n",
    "                span[\"text\"]\n",
    "                for line in block[\"lines\"]\n",
    "                for span in line[\"spans\"]\n",
    "            ).strip()\n",
    "            if \"Plaintiff\" in block_text and \"Defendant\" in block_text:\n",
    "                plaintiff_defendant = block_text\n",
    "                break\n",
    "\n",
    "    # Return all extracted metadata as a dictionary\n",
    "    return {\n",
    "        \"core term\": core_terms,\n",
    "        \"judges\": judges,\n",
    "        \"plaintiff_defendant\": plaintiff_defendant,\n",
    "        \"opinion by\": opinion_by,\n",
    "        \"prior history\": prior_history,\n",
    "        \"subsequent history\": subsequent_history\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'core term': ['Bui', 'infringement', 'cases', 'default judgment', 'statutory damages', \"attorney's fees\", 'hourly rate', 'injunction', 'costs', 'Courts', 'copyright infringement', 'factors', 'copied', 'permanent', 'default', 'allegations', 'distributed', 'subscriber', 'requests', 'skill', 'weigh'], 'judges': 'Honorable Richard A. Jones, United States District Judge.', 'plaintiff_defendant': 'Dallas Buyers Club, LLC, Plaintiff: David Allen Lowe, LOWE GRAHAM JONES, SEATTLE, WA', 'opinion by': 'Richard A. Jones', 'prior history': 'Dallas Buyers Club, LLC v. Doe, 2015 U.S. Dist. LEXIS 87450 (W.D. Wash., July 1, 2015)', 'subsequent history': 'Motion granted by, Judgment entered by Dallas Buyers Club, LLC v. Nydam, 2016 U.S. Dist.  LEXIS 184269 (W.D. Wash., Aug. 8, 2016)'}\n"
     ]
    }
   ],
   "source": [
    "print(extract_case_metadata_fixed_opinion(\"C:/Users/owoyi/Downloads/ÁõßÊïôÊéà/Â∞àÂà©ÂàÜÊûê/data/testing/cp16-78.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_URI = \"MONGO_URI\"\n",
    "DB_NAME = \"copyright\"\n",
    "COLLECTION_NAME = \"index_todo\"\n",
    "\n",
    "# === Metadata extraction from a single page ===\n",
    "def extract_metadata_from_page(pdf_path, page_num):\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            if page_num >= len(doc):\n",
    "                raise ValueError(f\"Page {page_num} out of range for {pdf_path}\")\n",
    "            page = doc[page_num]\n",
    "            page_text = page.get_text(\"text\")\n",
    "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "\n",
    "        def extract_field(pattern):\n",
    "            match = re.search(rf\"{pattern}:\\s*(.+)\", page_text)\n",
    "            return match.group(1).strip() if match else \"\"\n",
    "\n",
    "        def extract_section(pattern, end_patterns):\n",
    "            match = re.search(\n",
    "                rf'{pattern}\\s+([\\s\\S]+?)(?=\\n(?:{\"|\".join(end_patterns)}))',\n",
    "                page_text\n",
    "            )\n",
    "            if match:\n",
    "                return match.group(1).replace('\\n', ' ').strip()\n",
    "            return \"\"\n",
    "\n",
    "        core_raw = extract_section(\"Core Terms\", [\"Counsel:\", \"Opinion by:\", \"Judges?:\"])\n",
    "        core_terms = [x.strip() for x in core_raw.split(',')] if core_raw else []\n",
    "\n",
    "        judges = extract_field(\"Judges?\")\n",
    "        opinion_by = extract_field(\"Opinion by\")\n",
    "        prior_history = extract_section(\"Prior History\", [\"Disposition:\", \"Core Terms\"])\n",
    "        subsequent_history = extract_section(\"Subsequent History\", [\"Prior History:\", \"Core Terms\"])\n",
    "\n",
    "        plaintiff_defendant = \"\"\n",
    "        counsel_match = re.search(\n",
    "            r'Counsel:\\s*\\[\\*?\\d*\\]\\s*For (.+?Plaintiff.*?)\\.\\s*',\n",
    "            page_text, re.DOTALL\n",
    "        )\n",
    "        if counsel_match:\n",
    "            plaintiff_defendant = counsel_match.group(1).replace('\\n', ' ').strip()\n",
    "        else:\n",
    "            for block in blocks:\n",
    "                if \"lines\" not in block:\n",
    "                    continue\n",
    "                text = \" \".join(span[\"text\"] for line in block[\"lines\"] for span in line[\"spans\"]).strip()\n",
    "                if \"Plaintiff\" in text and \"Defendant\" in text:\n",
    "                    plaintiff_defendant = text\n",
    "                    break\n",
    "\n",
    "        return {\n",
    "            \"core term\": core_terms,\n",
    "            \"judges\": judges,\n",
    "            \"plaintiff_defendant\": plaintiff_defendant,\n",
    "            \"opinion by\": opinion_by,\n",
    "            \"prior history\": prior_history,\n",
    "            \"subsequent history\": subsequent_history\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to extract from {pdf_path}, page {page_num}: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Main process: update MongoDB ===\n",
    "def process_index_collection():\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[DB_NAME]\n",
    "    col = db[COLLECTION_NAME]\n",
    "\n",
    "    errors = []\n",
    "    for doc in col.find():\n",
    "        pdf = doc.get(\"pdf\")\n",
    "        page = doc.get(\"page\")\n",
    "        _id = doc[\"_id\"]\n",
    "\n",
    "        try:\n",
    "            if not pdf or page is None:\n",
    "                raise ValueError(\"Missing 'pdf' or 'page' field\")\n",
    "\n",
    "            pdf_path = os.path.join(\"data\", pdf)\n",
    "            if not os.path.exists(pdf_path):\n",
    "                raise FileNotFoundError(f\"File not found: {pdf_path}\")\n",
    "\n",
    "            metadata = extract_metadata_from_page(pdf_path, page)\n",
    "            col.update_one({\"_id\": _id}, {\"$set\": metadata})\n",
    "            print(f\"‚úÖ Updated {_id} ({pdf}, page {page})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_entry = {\n",
    "                \"id\": str(_id),\n",
    "                \"pdf\": pdf,\n",
    "                \"page\": page,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            print(f\"‚ùå Error: {error_entry}\")\n",
    "            errors.append(error_entry)\n",
    "\n",
    "    with open(\"error_log.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(errors, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_index_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# MongoDB config\n",
    "MONGO_URI = \"mongodb://yihua:Yh%40copyright@140.117.75.100:27017/?authSource=copyright\"\n",
    "DB_NAME = \"copyright\"\n",
    "COLLECTION_NAME = \"index_todo\"\n",
    "\n",
    "# ÊäΩÂèñÂñÆÈ†Å metadata\n",
    "# Finalized robust extraction function for legacy-style PDFs\n",
    "def extract_metadata_from_page_robust(pdf_path, page_num):\n",
    "    import fitz\n",
    "    import re\n",
    "\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        if page_num >= len(doc):\n",
    "            raise ValueError(f\"Page {page_num} out of range for {pdf_path}\")\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text(\"text\")\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        lines = [line[\"spans\"][0][\"text\"].strip() for block in blocks if \"lines\" in block for line in block[\"lines\"]]\n",
    "\n",
    "    # --- 1. Prior History (single-line version) ---\n",
    "    prior_history = \"\"\n",
    "    match_prior = re.search(r'Prior History:\\s*\\[*\\*?\\d*\\]*\\s*(.+)', text)\n",
    "    if match_prior:\n",
    "        prior_history = match_prior.group(1).strip()\n",
    "\n",
    "    # --- 2. Core Terms (appears on line \"Core Terms\", followed by next line list) ---\n",
    "    core_terms = []\n",
    "    try:\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip().startswith(\"Core Terms\"):\n",
    "                core_line = lines[i + 1].strip() if i + 1 < len(lines) else \"\"\n",
    "                core_terms = [x.strip() for x in core_line.split(',') if x.strip()]\n",
    "                break\n",
    "    except Exception:\n",
    "        core_terms = []\n",
    "\n",
    "    # --- 3. Counsel (multi-line after \"Counsel:\") ---\n",
    "    plaintiff_defendant = \"\"\n",
    "    try:\n",
    "        match = re.search(r'Counsel:\\s*(.+?)(?:\\n[A-Z][^\\n]*:|\\nLexisNexis|\\n+)', text, re.DOTALL)\n",
    "        if match:\n",
    "            block = match.group(1).replace('\\n', ' ').strip()\n",
    "            plaintiff_defendant = re.sub(r'\\s+', ' ', block)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # --- 4. Judges ---\n",
    "    judges = \"\"\n",
    "    match_judge = re.search(r'Judges?:\\s*(.+?Judge\\.)', text)\n",
    "    if match_judge:\n",
    "        judges = match_judge.group(1).strip()\n",
    "\n",
    "    # --- 5. Opinion by ---\n",
    "    opinion_by = \"\"\n",
    "    match_op = re.search(r'Opinion by:\\s*(.+)', text)\n",
    "    if match_op:\n",
    "        opinion_by = match_op.group(1).strip()\n",
    "\n",
    "    # --- 6. Subsequent History ---\n",
    "    subsequent_history = \"\"\n",
    "    match_sub = re.search(r'Subsequent History:\\s*(.+?)\\n', text)\n",
    "    if match_sub:\n",
    "        subsequent_history = match_sub.group(1).strip()\n",
    "\n",
    "    return {\n",
    "        \"core term\": core_terms,\n",
    "        \"judges\": judges,\n",
    "        \"plaintiff_defendant\": plaintiff_defendant,\n",
    "        \"opinion by\": opinion_by,\n",
    "        \"prior history\": prior_history,\n",
    "        \"subsequent history\": subsequent_history\n",
    "    }\n",
    "\n",
    "# ‰∏ªÁ®ãÂºèÔºöÊï¥ÊâπÊì∑Âèñ‰∏¶ÂØ´Âá∫CSV\n",
    "def extract_all_metadata_to_csv():\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[DB_NAME]\n",
    "    col = db[COLLECTION_NAME]\n",
    "\n",
    "    data = []\n",
    "    errors = []\n",
    "\n",
    "    for doc in col.find():\n",
    "        pdf = doc.get(\"pdf\")\n",
    "        page = doc.get(\"page\")\n",
    "        _id = doc[\"_id\"]\n",
    "\n",
    "        try:\n",
    "            if not pdf or page is None:\n",
    "                raise ValueError(\"Missing 'pdf' or 'page'\")\n",
    "\n",
    "            pdf_path = os.path.join(\"data\", pdf)\n",
    "            if not os.path.exists(pdf_path):\n",
    "                raise FileNotFoundError(f\"File not found: {pdf_path}\")\n",
    "\n",
    "            metadata = extract_metadata_from_page(pdf_path, page)\n",
    "            metadata[\"pdf\"] = pdf\n",
    "            metadata[\"page\"] = page\n",
    "            data.append(metadata)\n",
    "\n",
    "        except Exception as e:\n",
    "            errors.append({\n",
    "                \"id\": str(_id),\n",
    "                \"pdf\": pdf,\n",
    "                \"page\": page,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "\n",
    "    # Ëº∏Âá∫ÊàêÂäüË≥áÊñô\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"metadata_preview.csv\", index=False)\n",
    "    print(\"‚úÖ Â∑≤Ëº∏Âá∫ metadata_preview.csv\")\n",
    "\n",
    "    # ÈåØË™§Êó•Ë™å\n",
    "    if errors:\n",
    "        with open(\"error_log_preview.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(errors, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"‚ö† Êúâ {len(errors)} Á≠ÜÈåØË™§ÔºåÂ∑≤ÂÑ≤Â≠òËá≥ error_log_preview.json\")\n",
    "\n",
    "# Âü∑Ë°å‰∏ªÁ®ãÂºè\n",
    "if __name__ == \"__main__\":\n",
    "    extract_all_metadata_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_case_title_by_font(blocks):\n",
    "    candidate = \"\"\n",
    "    max_size = 0\n",
    "\n",
    "    for block in blocks:\n",
    "        if \"lines\" not in block:\n",
    "            continue\n",
    "        for line in block[\"lines\"]:\n",
    "            for span in line[\"spans\"]:\n",
    "                text = span[\"text\"].strip()\n",
    "                if \"v.\" in text and span[\"size\"] > max_size:\n",
    "                    max_size = span[\"size\"]\n",
    "                    candidate = text\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final version with extended page scan for 'Counsel' and 'Opinion by' extraction\n",
    "\n",
    "def extract_case_metadata_from_page_fixed(pdf_file_path, page_num):\n",
    "    import fitz\n",
    "    import re\n",
    "\n",
    "    with fitz.open(pdf_file_path) as doc:\n",
    "        # Read the main page (page_num) and one more for local metadata blocks\n",
    "        pages_text = \"\\n\".join([\n",
    "            doc[p].get_text(\"text\")\n",
    "            for p in range(page_num, min(page_num + 2, len(doc)))\n",
    "        ])\n",
    "\n",
    "        # Read an extended range (up to 4 pages) for Counsel and Opinion by\n",
    "        extended_text = \"\\n\".join([\n",
    "            doc[p].get_text(\"text\")\n",
    "            for p in range(page_num, min(page_num + 13, len(doc)))\n",
    "        ])\n",
    "\n",
    "        first_page_blocks = doc[page_num].get_text(\"dict\")[\"blocks\"]\n",
    "\n",
    "    def extract_with_regex(pattern, end_patterns, text, max_len=1000):\n",
    "        try:\n",
    "            match = re.search(\n",
    "                rf'{pattern}\\s+([\\s\\S]+?)(?=\\n(?:{\"|\".join(end_patterns)}))',\n",
    "                text\n",
    "            )\n",
    "            if match:\n",
    "                content = match.group(1).replace('\\n', ' ').strip()\n",
    "                return content if len(content) <= max_len else \"\"\n",
    "        except:\n",
    "            return \"\"\n",
    "        return \"\"\n",
    "\n",
    "    # Core Terms from 1‚Äì2 pages\n",
    "    core_terms_raw = extract_with_regex(\"Core Terms\", [\n",
    "        \"Counsel:\", \"LexisNexis\", \"HN\\\\d\\\\[\", \"Headnotes\", \"Opinion by:\", \"Judges?:\"\n",
    "    ], pages_text)\n",
    "    core_terms = [term.strip() for term in core_terms_raw.split(',')] if core_terms_raw else []\n",
    "\n",
    "    judges = extract_with_regex(\"Judges?:\", [\"Opinion by:\", \"Core Terms\", \"Counsel:\"], pages_text, max_len=300)\n",
    "\n",
    "    # Opinion by (extended scan)\n",
    "    opinion_by = \"\"\n",
    "    try:\n",
    "        op_match = re.search(r'Opinion by:\\s*(.+)', extended_text)\n",
    "        if op_match:\n",
    "            opinion_by = op_match.group(1).strip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Prior History (local)\n",
    "    prior_history = \"\"\n",
    "    try:\n",
    "        match_prior = re.search(r'Prior History:\\s*(.+)', pages_text)\n",
    "        if match_prior:\n",
    "            prior_history = re.sub(r'^\\[\\*\\*\\d+\\]\\s*', '', match_prior.group(1).strip())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    subsequent_history = extract_with_regex(\"Subsequent History:\", [\"Prior History:\", \"Disposition:\", \"Core Terms\"], pages_text, 1000)\n",
    "\n",
    "    # --- Plaintiff/Defendant Logic ---\n",
    "    def extract_case_title_by_font(blocks):\n",
    "        candidate = \"\"\n",
    "        max_size = 0\n",
    "        for block in blocks:\n",
    "            if \"lines\" not in block:\n",
    "                continue\n",
    "            for line in block[\"lines\"]:\n",
    "                for span in line[\"spans\"]:\n",
    "                    text = span[\"text\"].strip()\n",
    "                    if \"v.\" in text and span[\"size\"] > max_size:\n",
    "                        candidate = text\n",
    "                        max_size = span[\"size\"]\n",
    "        return candidate\n",
    "\n",
    "    plaintiff_defendant = \"\"\n",
    "    try:\n",
    "        match = re.search(\n",
    "            r'Counsel:\\s*\\[\\*?\\d*\\]\\s*For (.+?Plaintiff.*?)\\.\\s*',\n",
    "            extended_text, re.DOTALL\n",
    "        )\n",
    "        if match:\n",
    "            plaintiff_defendant = match.group(1).replace('\\n', ' ').strip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    all_block_texts = []\n",
    "    for block in first_page_blocks:\n",
    "        if \"lines\" in block:\n",
    "            block_text = \" \".join(span[\"text\"] for line in block[\"lines\"] for span in line[\"spans\"]).strip()\n",
    "            all_block_texts.append(block_text)\n",
    "\n",
    "    if not plaintiff_defendant:\n",
    "        for block_text in all_block_texts:\n",
    "            if re.match(r'^[A-Z0-9,\\.\\-\\(\\)\\s]+v\\.', block_text):\n",
    "                plaintiff_defendant = block_text\n",
    "                break\n",
    "\n",
    "    if not plaintiff_defendant:\n",
    "        for block_text in all_block_texts:\n",
    "            if \"Plaintiff\" in block_text and \"Defendant\" in block_text:\n",
    "                plaintiff_defendant = block_text\n",
    "                break\n",
    "\n",
    "    if not plaintiff_defendant:\n",
    "        plaintiff_defendant = extract_case_title_by_font(first_page_blocks)\n",
    "\n",
    "    return {\n",
    "        \"core term\": core_terms,\n",
    "        \"judges\": judges,\n",
    "        \"plaintiff_defendant\": plaintiff_defendant,\n",
    "        \"opinion by\": opinion_by,\n",
    "        \"prior history\": prior_history,\n",
    "        \"subsequent history\": subsequent_history\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = extract_case_metadata_from_page_fixed(\"data/cp13.pdf\", 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'core term': ['Programs',\n",
       "  'infringement',\n",
       "  'statutory damages',\n",
       "  'default',\n",
       "  'default judgment',\n",
       "  'advertising',\n",
       "  'distributing',\n",
       "  'copies',\n",
       "  'copyright  infringement',\n",
       "  'products',\n",
       "  'unauthorized',\n",
       "  'damages',\n",
       "  'manufacturing',\n",
       "  'Importing',\n",
       "  'reproduce',\n",
       "  'picture',\n",
       "  'merits',\n",
       "  'personal  jurisdiction',\n",
       "  'permanent injunction',\n",
       "  'offering',\n",
       "  'factors',\n",
       "  'selling',\n",
       "  'notice',\n",
       "  'injunction',\n",
       "  'entry of default',\n",
       "  'actual damage',\n",
       "  'certificate',\n",
       "  'counterfeit',\n",
       "  'similarity',\n",
       "  'considers'],\n",
       " 'judges': 'Fernando M. Olguin, United States District Judge.',\n",
       " 'plaintiff_defendant': 'Warner Bros Home Entertainment Inc, Plaintiff: James Andrew Coombs, Nicole L Drey, J  Andrew Coombs APC, Glendale, CA',\n",
       " 'opinion by': 'Fernando M. Olguin',\n",
       " 'prior history': \"Warner Bros Home Entm't v. Jimenez, 2013 U.S. Dist. LEXIS 37212 (C.D. Cal., Mar. 18, 2013)\",\n",
       " 'subsequent history': ''}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Ë®≠ÂÆö MongoDB ===\n",
    "client = MongoClient(\"mongodb://yihua:Yh%40copyright@140.117.75.100:27017/?authSource=copyright\")\n",
    "db = client[\"copyright\"]\n",
    "collection = db[\"index_todo\"]\n",
    "\n",
    "# === ÊîæÂÖ•‰Ω†ÂÆåÊï¥ÁöÑÂáΩÊï∏ extract_case_metadata_from_page_fixed ===\n",
    "# ÂèØÂæûÂâçÈù¢Ë§áË£Ω\n",
    "\n",
    "# === ÊâπÊ¨°ËôïÁêÜ‰∏¶ÂØ´Âõû MongoDB ===\n",
    "errors = []\n",
    "\n",
    "for doc in tqdm(collection.find()):\n",
    "    pdf = doc.get(\"pdf\")\n",
    "    page = doc.get(\"page\")\n",
    "    _id = doc.get(\"_id\")\n",
    "\n",
    "    try:\n",
    "        if not pdf or page is None:\n",
    "            raise ValueError(\"Missing 'pdf' or 'page'\")\n",
    "        corrected_page = page - 1\n",
    "        pdf_path = os.path.join(\"data\", pdf)\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"PDF not found: {pdf_path}\")\n",
    "\n",
    "        metadata = extract_case_metadata_from_page_fixed(pdf_path, corrected_page)\n",
    "\n",
    "        # === Êõ¥Êñ∞Ë≥áÊñôÂõûÂéü collection ===\n",
    "        collection.update_one(\n",
    "            {\"_id\": _id},\n",
    "            {\"$set\": metadata}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        errors.append({\"_id\": str(_id), \"pdf\": pdf, \"page\": page, \"error\": str(e)})\n",
    "\n",
    "# === ÈåØË™§Á¥ÄÈåÑÔºàÂèØÈÅ∏Ôºâ===\n",
    "if errors:\n",
    "    import json\n",
    "    with open(\"writeback_errors.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(errors, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚ö†Ô∏è ÂÆåÊàêÔºå‰ΩÜÊúâ {len(errors)} Á≠ÜÈåØË™§ÔºåË´ãÊ™¢Êü• writeback_errors.json\")\n",
    "else:\n",
    "    print(\"‚úÖ ÊâÄÊúâË≥áÊñôÂ∑≤ÊàêÂäüÂØ´ÂÖ• MongoDBÔºÅ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_prior_history(text):\n",
    "    \"\"\"\n",
    "    ÂæûÊèê‰æõÁöÑ page_text ‰∏≠ÊèêÂèñ Prior History ÂçÄÊÆµÂÖßÂÆπ„ÄÇ\n",
    "    Êé°Áî®Êõ¥Á©©ÂÆöÁöÑÊ≠£ÂâáÈÇèËºØÔºåÈÅøÂÖçË¢´ÊÆµËêΩÂàáÊñ∑„ÄÇ\n",
    "    \"\"\"\n",
    "    pattern = r\"Prior History[:\\s]+([\\s\\S]+?)(?=\\n(?:Disposition:|Core Terms))\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        content = match.group(1).replace('\\n', ' ').strip()\n",
    "        return content\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appeal from the United States District Court for the Eastern District of Pennsylvania. (D.C. Civil Action No. 2-10-cv-02680). District Judge: Honorable J. Curtis Joyner. Am. Bd. of Internal Med. v. Muller, 2012 U.S. Dist. LEXIS 123481 (E.D. Pa., Aug. 29, 2012)\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "Prior History\n",
    "Appeal from the United States District Court for the Eastern District of Pennsylvania.\n",
    "(D.C. Civil Action No. 2-10-cv-02680). District Judge: Honorable J. Curtis Joyner.\n",
    "Am. Bd. of Internal Med. v. Muller, 2012 U.S. Dist. LEXIS 123481 (E.D. Pa., Aug. 29, 2012)\n",
    "\n",
    "Disposition: Judgment of the district court affirmed.\n",
    "\"\"\"\n",
    "\n",
    "print(extract_prior_history(sample_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[**1] APPEAL FROM THE UNITED STATES DISTRICT COURT FOR THE WESTERN DISTRICT OF OKLAHOMA. (D.C. No. CV-93-1212-R). David L. Russell, District Judge.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "Prior History: [**1] APPEAL FROM THE UNITED STATES DISTRICT COURT FOR THE WESTERN DISTRICT\n",
    "OF OKLAHOMA. (D.C. No. CV-93-1212-R). David L. Russell, District Judge.\n",
    "Disposition: AFFIRMED.\n",
    "\"\"\"\n",
    "\n",
    "print(extract_prior_history(sample_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[**1] Appeals from the United States District Court for the District of Minnesota. CIV 4-87-454. Honorable David S. Doty, District Judge. Honorable James M. Rosenbaum, District Judge.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "Prior History: [**1] Appeals from the United States District Court for the District of Minnesota. CIV 4-87-454.\n",
    "Honorable David S. Doty, District Judge. Honorable James M. Rosenbaum, District Judge.\n",
    "Disposition: Affirmed.\n",
    "\"\"\"\n",
    "\n",
    "print(extract_prior_history(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "# === MongoDB config ===\n",
    "MONGO_URI = \"mongodb://yihua:Yh%40copyright@140.117.75.100:27017/?authSource=copyright\"\n",
    "DB_NAME = \"copyright\"\n",
    "COLLECTION_NAME = \"index_todo\"\n",
    "\n",
    "# === PDF folder path ===\n",
    "PDF_DIR = \"./data\"\n",
    "\n",
    "# === ÊèêÂèñ Prior History ÁöÑÂáΩÊï∏ ===\n",
    "def extract_prior_history(text):\n",
    "    pattern = r\"Prior History[:\\s]+([\\s\\S]+?)(?=\\n(?:Disposition:|Core Terms))\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group(1).replace('\\n', ' ').strip()\n",
    "    return \"\"\n",
    "\n",
    "# === ÈÄ£Êé• MongoDB ===\n",
    "client = MongoClient(MONGO_URI)\n",
    "collection = client[DB_NAME][COLLECTION_NAME]\n",
    "\n",
    "# === ËôïÁêÜÊØè‰∏ÄÁ≠ÜÊñá‰ª∂ ===\n",
    "error_logs = []\n",
    "\n",
    "docs = collection.find({\n",
    "    \"pdf\": {\"$exists\": True},\n",
    "    \"page\": {\"$type\": \"int\"}\n",
    "})\n",
    "\n",
    "for doc in docs:\n",
    "    pdf_filename = doc[\"pdf\"]\n",
    "    start_page = doc[\"page\"]\n",
    "    page_index = start_page - 1 \n",
    "    _id = doc[\"_id\"]\n",
    "    pdf_path = os.path.join(PDF_DIR, pdf_filename)\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"{pdf_path} not found\")\n",
    "\n",
    "        with fitz.open(pdf_path) as doc_pdf:\n",
    "            if page_index >= len(doc_pdf):\n",
    "                raise IndexError(f\"Page {page_index} out of range in {pdf_filename}\")\n",
    "            text = doc_pdf[page_index].get_text(\"text\")\n",
    "\n",
    "        prior = extract_prior_history(text)\n",
    "\n",
    "        # Êõ¥Êñ∞ MongoDB\n",
    "        collection.update_one(\n",
    "            {\"_id\": _id},\n",
    "            {\"$set\": {\"prior history\": prior}}\n",
    "        )\n",
    "        print(f\"Updated {_id} with prior history: {prior[:50]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {_id}: {e}\")\n",
    "        error_logs.append({\"_id\": _id, \"error\": str(e)})\n",
    "\n",
    "# === ÂèØÈÅ∏ÔºöËº∏Âá∫ÈåØË™§Á¥ÄÈåÑ ===\n",
    "if error_logs:\n",
    "    with open(\"prior_history_errors.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        import json\n",
    "        json.dump(error_logs, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\n‚ö†Ô∏è Saved error logs to prior_history_errors.json\")\n",
    "\n",
    "print(\"\\n‚úÖ All done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Cleaned and updated 301 documents.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import re\n",
    "\n",
    "# === MongoDB config ===\n",
    "MONGO_URI = \"mongodb://yihua:Yh%40copyright@140.117.75.100:27017/?authSource=copyright\"\n",
    "DB_NAME = \"copyright\"\n",
    "COLLECTION_NAME = \"index_todo\"\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "collection = client[DB_NAME][COLLECTION_NAME]\n",
    "\n",
    "# Ê∏ÖÈô§ [*1], [**12] ÈÄôÈ°ûÊ®ôË®òÁöÑÊ≠£ÂâáË°®ÈÅîÂºè\n",
    "footnote_pattern = re.compile(r'\\[\\*+\\d+\\]')\n",
    "\n",
    "def clean_text(text):\n",
    "    return footnote_pattern.sub('', text).strip()\n",
    "\n",
    "def clean_field(field):\n",
    "    if isinstance(field, str):\n",
    "        return clean_text(field)\n",
    "    elif isinstance(field, list):\n",
    "        return [clean_text(x) for x in field if isinstance(x, str)]\n",
    "    return field\n",
    "\n",
    "# Êü•Ë©¢Êúâ‰ªª‰∏ÄÊ¨Ñ‰ΩçÂ≠òÂú®ÁöÑÊñá‰ª∂\n",
    "docs = collection.find({\n",
    "    \"$or\": [\n",
    "        {\"prior history\": {\"$exists\": True, \"$ne\": None}},\n",
    "        {\"subsequent history\": {\"$exists\": True, \"$ne\": None}},\n",
    "        {\"opinion by\": {\"$exists\": True, \"$ne\": None}},\n",
    "        {\"judges\": {\"$exists\": True, \"$ne\": None}},\n",
    "    ]\n",
    "})\n",
    "\n",
    "updated = 0\n",
    "\n",
    "for doc in docs:\n",
    "    _id = doc[\"_id\"]\n",
    "    old_fields = {\n",
    "        \"prior history\": doc.get(\"prior history\", \"\"),\n",
    "        \"subsequent history\": doc.get(\"subsequent history\", \"\"),\n",
    "        \"opinion by\": doc.get(\"opinion by\", \"\"),\n",
    "        \"judges\": doc.get(\"judges\", \"\"),\n",
    "    }\n",
    "\n",
    "    new_fields = {k: clean_field(v) for k, v in old_fields.items()}\n",
    "\n",
    "    # ÊØîÂ∞çÊòØÂê¶ÊúâËÆäÂåñ\n",
    "    if new_fields != old_fields:\n",
    "        collection.update_one(\n",
    "            {\"_id\": _id},\n",
    "            {\"$set\": new_fields}\n",
    "        )\n",
    "        updated += 1\n",
    "\n",
    "print(f\"\\n‚úÖ Cleaned and updated {updated} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total unique judges: 1059\n",
      "üìÑ Saved to judges_raw.csv\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import csv\n",
    "\n",
    "# === MongoDB config ===\n",
    "MONGO_URI = \"mongodb://yihua:Yh%40copyright@140.117.75.100:27017/?authSource=copyright\"\n",
    "DB_NAME = \"copyright\"\n",
    "COLLECTION_NAME = \"index_todo\"\n",
    "\n",
    "# === ÈÄ£Êé• MongoDB ===\n",
    "client = MongoClient(MONGO_URI)\n",
    "collection = client[DB_NAME][COLLECTION_NAME]\n",
    "\n",
    "# === ÂÑ≤Â≠òÊâÄÊúâÊ≥ïÂÆòÂêçÂ≠óÁöÑ setÔºàÈÅøÂÖçÈáçË§áÔºâ===\n",
    "all_judges = set()\n",
    "\n",
    "# === Êü•Ë©¢ÊâÄÊúâÊúâ opinion by ÁöÑË≥áÊñô ===\n",
    "docs = collection.find({\n",
    "    \"opinion by\": {\"$exists\": True, \"$ne\": None}\n",
    "})\n",
    "\n",
    "for doc in docs:\n",
    "    raw = doc[\"opinion by\"]\n",
    "\n",
    "    if isinstance(raw, list):\n",
    "        names = raw\n",
    "    elif isinstance(raw, str):\n",
    "        if raw.strip():\n",
    "            names = [raw]\n",
    "        else:\n",
    "            names = []\n",
    "    else:\n",
    "        names = []\n",
    "\n",
    "    # Áµ±‰∏ÄÊ†ºÂºèÔºåtitle() ‰∏¶ÂéªÁ©∫ÁôΩ\n",
    "    formatted = [name.strip().title() for name in names if name.strip()]\n",
    "    all_judges.update(formatted)\n",
    "\n",
    "# === Â∞áÁµêÊûúÂØ´ÂÖ• CSV ===\n",
    "unique_judges = sorted(all_judges)\n",
    "\n",
    "csv_filename = \"judges_raw.csv\"\n",
    "with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"name\"])  # Ê®ôÈ°åÂàó\n",
    "    for name in unique_judges:\n",
    "        writer.writerow([name])\n",
    "\n",
    "print(f\"\\n‚úÖ Total unique judges: {len(unique_judges)}\")\n",
    "print(f\"üìÑ Saved to {csv_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "def extract_court_names(text):\n",
    "    return any(kw in text for kw in [\"Court\", \"Circuit\"])\n",
    "\n",
    "def scrape_ballotpedia_judge_info(judge_name):\n",
    "    # Convert name to Ballotpedia URL format\n",
    "    url_name = judge_name.strip().title().replace(\" \", \"_\")\n",
    "    url = f\"https://ballotpedia.org/{url_name}\"\n",
    "\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    box = soup.find('div', class_='infobox person')\n",
    "    data = {}\n",
    "    education = {}\n",
    "    circuit_history = []\n",
    "    Aliases = []\n",
    "\n",
    "    if not box:\n",
    "        data[\"error\"] = f\"No infobox found for {judge_name}\"\n",
    "        return data\n",
    "\n",
    "    rows = box.find_all('div', class_='widget-row')\n",
    "\n",
    "    # Name and current office (excluding party labels)\n",
    "    name_candidates = box.find_all('div', class_=lambda x: x and 'widget-row value-only' in x)\n",
    "    filtered = [div for div in name_candidates if div.get_text(strip=True) not in [\"Democratic Party\", \"Republican Party\", \"Nonpartisan\"]]\n",
    "    \n",
    "    # Get the official name from the page title\n",
    "    page_name_tag = soup.find('span', class_='mw-page-title-main')\n",
    "    if page_name_tag:\n",
    "        page_name = page_name_tag.get_text(strip=True)\n",
    "        data[\"Name\"] = page_name\n",
    "\n",
    "    # Check if judge_name (normalized) differs from the page name\n",
    "    normalized_input = judge_name.strip().lower().replace(\"_\", \"\").replace(\" \", \"\")\n",
    "    normalized_page = page_name.lower().replace(\" \", \"\")\n",
    "    if normalized_input != normalized_page:\n",
    "        Aliases.append(judge_name)\n",
    "\n",
    "    # Party\n",
    "    party_tag = box.find('a', href=lambda x: x and (\"Democratic_Party\" in x or \"Republican_Party\" in x or \"Nonpartisan\" in x))\n",
    "    if party_tag:\n",
    "        data[\"Party\"] = party_tag.get_text(strip=True)\n",
    "\n",
    "    # Main infobox parsing\n",
    "    for row in rows:\n",
    "        key_tag = row.find('div', class_='widget-key')\n",
    "        value_tag = row.find('div', class_='widget-value')\n",
    "\n",
    "        if key_tag and value_tag:\n",
    "            key = key_tag.get_text(strip=True)\n",
    "            val = value_tag.get_text(separator=' ', strip=True)\n",
    "\n",
    "            if key in [\"Bachelor's\", \"Law\"]:\n",
    "                education[key] = val\n",
    "            elif extract_court_names(key):\n",
    "                circuit_history.append(key)\n",
    "            elif extract_court_names(val):\n",
    "                circuit_history.append(val)\n",
    "            else:\n",
    "                data[key] = val\n",
    "\n",
    "    # Bold label above, value below\n",
    "    bold_divs = box.find_all('div', style=lambda x: x and 'font-weight: bold' in x)\n",
    "    for div in bold_divs:\n",
    "        key = div.get_text(strip=True)\n",
    "        next_div = div.find_next_sibling('div')\n",
    "        if next_div:\n",
    "            val = next_div.get_text(strip=True)\n",
    "            if extract_court_names(key):\n",
    "                circuit_history.append(key)\n",
    "            if extract_court_names(val):\n",
    "                circuit_history.append(val)\n",
    "            if not extract_court_names(key) and not extract_court_names(val):\n",
    "                data[key] = val\n",
    "\n",
    "    # Clean circuit entries\n",
    "    cleaned_circuits = list(set([\n",
    "        entry.strip() for entry in circuit_history if ':' not in entry\n",
    "    ]))\n",
    "\n",
    "    # Attach structured fields\n",
    "    if education:\n",
    "        data[\"Education\"] = education\n",
    "    if cleaned_circuits:\n",
    "        data[\"Circuit\"] = cleaned_circuits\n",
    "\n",
    "    # Paragraphs and gender detection\n",
    "    paragraphs = soup.find_all('p')\n",
    "    all_text = ' '.join(p.get_text(strip=True) for p in paragraphs).lower()\n",
    "    data[\"content\"] = all_text\n",
    "\n",
    "    she_count = all_text.count(' she ')\n",
    "    her_count = all_text.count(' her ')\n",
    "    he_count = all_text.count(' he ')\n",
    "    his_count = all_text.count(' his ')\n",
    "\n",
    "    if she_count + her_count > he_count + his_count:\n",
    "        data[\"Gender\"] = \"Female\"\n",
    "    elif he_count + his_count > 0:\n",
    "        data[\"Gender\"] = \"Male\"\n",
    "\n",
    "    data[\"Ballotpedia URL\"] = url\n",
    "    if Aliases:\n",
    "        data[\"Aliases\"] = Aliases\n",
    "\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "# ‚úÖ 1. ÈÄ£Á∑öÂà∞ MongoDB\n",
    "client = MongoClient(\"mongodb://yihua:Yh%40copyright@140.117.75.100:27017/?authSource=copyright\")  # Â¶ÇÊûú‰Ω†ÊúâÂ∏≥ÂØÜÊàñÈÅ†Á´Ø URIÔºåË´ãÊîπÊéâÈÄôË£°\n",
    "db = client[\"copyright\"]  \n",
    "collection = db[\"index_todo\"]\n",
    "\n",
    "# ‚úÖ 2. Êó•ÊúüÂ≠ó‰∏≤ËΩâÊèõÂáΩÂºè\n",
    "def try_parse_date(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%Y/%m/%d\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ‚úÖ 3. ËôïÁêÜÊØèÁ≠ÜÊñá‰ª∂\n",
    "for doc in collection.find():\n",
    "    updates = {}\n",
    "    for field in [\"Decided\", \"Others\", \"Argued\"]:\n",
    "        if field in doc and isinstance(doc[field], str):\n",
    "            parsed_date = try_parse_date(doc[field])\n",
    "            if parsed_date:\n",
    "                updates[field] = parsed_date\n",
    "\n",
    "    if updates:\n",
    "        collection.update_one({\"_id\": doc[\"_id\"]}, {\"$set\": updates})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo  # Python 3.9+\n",
    "\n",
    "client = MongoClient(\"mongodb://yihua:Yh%40copyright@140.117.75.100:27017/?authSource=copyright\")\n",
    "db = client[\"copyright\"]\n",
    "collection = db[\"index_todo\"]\n",
    "\n",
    "def to_gmt8(date_str):\n",
    "    try:\n",
    "        dt = datetime.strptime(date_str, \"%Y/%m/%d\")\n",
    "        return dt.replace(tzinfo=ZoneInfo(\"Asia/Taipei\"))  # ÊåáÂÆöÁÇ∫ GMT+8\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "for doc in collection.find():\n",
    "    updates = {}\n",
    "    for field in [\"Decided\", \"Others\", \"Argued\"]:\n",
    "        if field in doc and isinstance(doc[field], str):\n",
    "            dt = to_gmt8(doc[field])\n",
    "            if dt:\n",
    "                updates[field] = dt\n",
    "    if updates:\n",
    "        collection.update_one({\"_id\": doc[\"_id\"]}, {\"$set\": updates})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
